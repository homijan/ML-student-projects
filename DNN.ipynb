{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Open a new Jupyter notebook to implement this activity.\n",
    "#2. Import the TensorFlow and pandas libraries.\n",
    "#3. Load in the superconductivity.csv dataset.\n",
    "#4. Drop any rows that have null values.\n",
    "#5. Set the target as the critical_temp column and the feature dataset as the remaining columns.\n",
    "#6. Rescale the feature dataset using a standard scaler.\n",
    "#7. Initialize a model of the Keras Sequential class.\n",
    "#8. Add an input layer, four hidden layers of sizes 64, 32, 16, and 8, and an output layer of size 1 to the model. Add a ReLU activation function to the first hidden layer.\n",
    "#9. Compile the model with an RMSprop optimizer with a learning rate equal to 0.001 and the mean squared error for the loss.\n",
    "#10. Add a callback to write logs to TensorBoard. (optional)\n",
    "#11. Fit the model to the training data for 100 epochs, with a batch size equal to 32 and a validation split equal to 20%.\n",
    "#12. Evaluate the model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/fenago/datasets/main/superconductivity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear all empty values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions (x, y, #samples): (81, 1, 21263)\n"
     ]
    }
   ],
   "source": [
    "# Create target (y) and features (x)\n",
    "target = df[['critical_temp']]\n",
    "features = df.drop(['critical_temp'], axis=1)\n",
    "print(f'dimensions (x, y, #samples): {features.shape[1], target.shape[1], target.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale input data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "feature_array = scaler.fit_transform(features)\n",
    "features = pd.DataFrame(feature_array, columns=features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NN object\n",
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add input layer: size coresponds to the number of x components\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(features.shape[1],), name='Input_layer'))\n",
    "\n",
    "# Add hidden layers\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu', name='Dense_layer_1'))\n",
    "model.add(tf.keras.layers.Dense(32, name='Dense_layer_2'))\n",
    "model.add(tf.keras.layers.Dense(16, name='Dense_layer_3'))\n",
    "model.add(tf.keras.layers.Dense(8, name='Dense_layer_4'))\n",
    "\n",
    "# Add output layer: size corresponds to the number of y components\n",
    "model.add(tf.keras.layers.Dense(target.shape[1], name='Output_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set back propagation learner/optimizer \n",
    "model.compile(tf.optimizers.RMSprop(0.001), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for tensor board\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/532 [..............................] - ETA: 0s - loss: 2160.8083WARNING:tensorflow:From /opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0054s vs `on_train_batch_end` time: 0.0274s). Check your callbacks.\n",
      "532/532 [==============================] - 1s 2ms/step - loss: 602.2785 - val_loss: 231.4569\n",
      "Epoch 2/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 379.8764 - val_loss: 308.1671\n",
      "Epoch 3/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 339.3413 - val_loss: 230.4368\n",
      "Epoch 4/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 327.0775 - val_loss: 252.5370\n",
      "Epoch 5/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 319.1301 - val_loss: 205.0048\n",
      "Epoch 6/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 308.8497 - val_loss: 523.0133\n",
      "Epoch 7/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 299.6206 - val_loss: 244.2511\n",
      "Epoch 8/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 294.8822 - val_loss: 206.2517\n",
      "Epoch 9/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 289.9684 - val_loss: 279.6464\n",
      "Epoch 10/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 285.6806 - val_loss: 187.0549\n",
      "Epoch 11/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 281.9894 - val_loss: 214.5072\n",
      "Epoch 12/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 275.9124 - val_loss: 305.2103\n",
      "Epoch 13/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 273.9520 - val_loss: 266.4243\n",
      "Epoch 14/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 270.4785 - val_loss: 217.2614\n",
      "Epoch 15/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 268.1871 - val_loss: 225.0523\n",
      "Epoch 16/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 265.5141 - val_loss: 213.9614\n",
      "Epoch 17/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 260.5262 - val_loss: 202.4616\n",
      "Epoch 18/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 263.1783 - val_loss: 192.5933\n",
      "Epoch 19/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 259.3976 - val_loss: 290.3455\n",
      "Epoch 20/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 256.9996 - val_loss: 337.0719\n",
      "Epoch 21/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 255.5537 - val_loss: 204.0714\n",
      "Epoch 22/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 252.7671 - val_loss: 210.6057\n",
      "Epoch 23/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 250.8484 - val_loss: 239.9030\n",
      "Epoch 24/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 248.8745 - val_loss: 333.0838\n",
      "Epoch 25/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 245.8179 - val_loss: 197.3162\n",
      "Epoch 26/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 248.5771 - val_loss: 223.6444\n",
      "Epoch 27/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 244.7631 - val_loss: 220.9560\n",
      "Epoch 28/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 243.8744 - val_loss: 210.7126\n",
      "Epoch 29/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 244.7216 - val_loss: 299.5537\n",
      "Epoch 30/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 242.0199 - val_loss: 309.1800\n",
      "Epoch 31/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 241.2410 - val_loss: 202.5973\n",
      "Epoch 32/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 240.5535 - val_loss: 216.1619\n",
      "Epoch 33/100\n",
      "532/532 [==============================] - 1s 2ms/step - loss: 238.9609 - val_loss: 204.1234\n",
      "Epoch 34/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 237.4161 - val_loss: 205.5878\n",
      "Epoch 35/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 236.7419 - val_loss: 193.1911\n",
      "Epoch 36/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 237.3594 - val_loss: 226.6439\n",
      "Epoch 37/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 235.3745 - val_loss: 239.9887\n",
      "Epoch 38/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 234.1695 - val_loss: 242.4613\n",
      "Epoch 39/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 232.0289 - val_loss: 251.4917\n",
      "Epoch 40/100\n",
      "532/532 [==============================] - 1s 2ms/step - loss: 233.1307 - val_loss: 236.8684\n",
      "Epoch 41/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 232.7961 - val_loss: 194.5516\n",
      "Epoch 42/100\n",
      "532/532 [==============================] - 1s 2ms/step - loss: 231.3314 - val_loss: 242.3615\n",
      "Epoch 43/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 229.2237 - val_loss: 232.9487\n",
      "Epoch 44/100\n",
      "532/532 [==============================] - 1s 2ms/step - loss: 228.8695 - val_loss: 261.5381\n",
      "Epoch 45/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 228.2078 - val_loss: 219.9681\n",
      "Epoch 46/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 226.6100 - val_loss: 224.7818\n",
      "Epoch 47/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 228.2228 - val_loss: 223.0353\n",
      "Epoch 48/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 224.6519 - val_loss: 219.2130\n",
      "Epoch 49/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 225.7701 - val_loss: 277.0151\n",
      "Epoch 50/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 225.4534 - val_loss: 243.2106\n",
      "Epoch 51/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 225.5408 - val_loss: 208.4092\n",
      "Epoch 52/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 222.7585 - val_loss: 255.7008\n",
      "Epoch 53/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 222.9853 - val_loss: 274.5518\n",
      "Epoch 54/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 220.9785 - val_loss: 253.6163\n",
      "Epoch 55/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 220.9935 - val_loss: 211.8046\n",
      "Epoch 56/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 222.6784 - val_loss: 242.5544\n",
      "Epoch 57/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 219.8016 - val_loss: 325.8629\n",
      "Epoch 58/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 218.8749 - val_loss: 275.4648\n",
      "Epoch 59/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 220.0582 - val_loss: 250.5036\n",
      "Epoch 60/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 220.0594 - val_loss: 310.3189\n",
      "Epoch 61/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 216.3800 - val_loss: 244.6049\n",
      "Epoch 62/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 219.0298 - val_loss: 250.4261\n",
      "Epoch 63/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 218.1374 - val_loss: 324.0540\n",
      "Epoch 64/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 215.5331 - val_loss: 218.3703\n",
      "Epoch 65/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 216.2483 - val_loss: 249.0966\n",
      "Epoch 66/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 215.8415 - val_loss: 265.5746\n",
      "Epoch 67/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 215.6720 - val_loss: 376.0171\n",
      "Epoch 68/100\n",
      "532/532 [==============================] - 1s 2ms/step - loss: 212.8456 - val_loss: 239.5952\n",
      "Epoch 69/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 213.1475 - val_loss: 274.5630\n",
      "Epoch 70/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 214.5271 - val_loss: 204.9229\n",
      "Epoch 71/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 211.5916 - val_loss: 228.5190\n",
      "Epoch 72/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 212.1388 - val_loss: 294.7268\n",
      "Epoch 73/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 210.9120 - val_loss: 224.0547\n",
      "Epoch 74/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 212.4078 - val_loss: 261.9183\n",
      "Epoch 75/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 210.8530 - val_loss: 243.8659\n",
      "Epoch 76/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 210.0210 - val_loss: 243.5338\n",
      "Epoch 77/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 209.5991 - val_loss: 232.3567\n",
      "Epoch 78/100\n",
      "532/532 [==============================] - 1s 2ms/step - loss: 209.0430 - val_loss: 274.6566\n",
      "Epoch 79/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 208.9417 - val_loss: 298.8226\n",
      "Epoch 80/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 210.2041 - val_loss: 306.9171\n",
      "Epoch 81/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 210.2780 - val_loss: 244.3374\n",
      "Epoch 82/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 209.1304 - val_loss: 252.7041\n",
      "Epoch 83/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 207.6798 - val_loss: 235.6339\n",
      "Epoch 84/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 206.8766 - val_loss: 314.1382\n",
      "Epoch 85/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 208.8221 - val_loss: 227.3133\n",
      "Epoch 86/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 207.0011 - val_loss: 284.4311\n",
      "Epoch 87/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 207.1942 - val_loss: 227.0105\n",
      "Epoch 88/100\n",
      "532/532 [==============================] - 1s 1ms/step - loss: 206.3429 - val_loss: 237.7585\n",
      "Epoch 89/100\n",
      "532/532 [==============================] - 1s 2ms/step - loss: 205.2534 - val_loss: 221.2150\n",
      "Epoch 90/100\n",
      "450/532 [========================>.....] - ETA: 0s - loss: 207.8145"
     ]
    }
   ],
   "source": [
    "# Actual training\n",
    "model.fit(x=features.to_numpy(), y=target.to_numpy(),epochs=100, callbacks=[tensorboard_callback] , validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate how well is our model trained\n",
    "loss = model.evaluate(features.to_numpy(), target.to_numpy())\n",
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
